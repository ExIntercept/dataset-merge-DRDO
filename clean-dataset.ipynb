{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312d7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_m2_file(m2_path):\n",
    "    \"\"\"\n",
    "    Parses an M2 file and returns a list of (source, target) sentence pairs.\n",
    "    Each block in M2 corresponds to one sentence and its edits.\n",
    "    \"\"\"\n",
    "    with open(m2_path, 'r', encoding='utf-8') as f:\n",
    "        blocks = f.read().strip().split(\"\\n\\n\")\n",
    "\n",
    "    pairs = []\n",
    "    for block in tqdm(blocks, desc=f\"Parsing {m2_path}\"):\n",
    "        lines = block.strip().split(\"\\n\")\n",
    "        if not lines or not lines[0].startswith(\"S \"):\n",
    "            continue\n",
    "        original = lines[0][2:].strip().split()\n",
    "        edits = [line for line in lines[1:] if line.startswith(\"A \") and \"|||\" in line]\n",
    "\n",
    "        # No edits — skip or treat as identity\n",
    "        if not edits:\n",
    "            corrected = original\n",
    "        else:\n",
    "            corrected = original[:]\n",
    "            offset = 0  # Track position shift from edits\n",
    "            for edit in edits:\n",
    "                parts = edit.split(\"|||\")\n",
    "                span = list(map(int, parts[0][2:].split()))\n",
    "                correction = parts[2].strip()\n",
    "                if correction == \"-NONE-\":\n",
    "                    continue\n",
    "                start, end = span[0], span[1]\n",
    "                corrected[start + offset:end + offset] = correction.split()\n",
    "                offset += len(correction.split()) - (end - start)\n",
    "\n",
    "        # Add prefix \"gec: \" to input for T5\n",
    "        source = \" \".join(original)\n",
    "        target = \" \".join(corrected)\n",
    "        pairs.append((\"gec: \" + source, target))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing wi+locness/m2/ABC.train.gold.bea19.m2: 100%|██████████| 34308/34308 [00:00<00:00, 117804.29it/s]\n",
      "Parsing wi+locness/m2/ABCN.dev.gold.bea19.m2: 100%|██████████| 4384/4384 [00:00<00:00, 106872.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_pairs_wi = parse_m2_file(\"wi+locness/m2/ABC.train.gold.bea19.m2\")\n",
    "val_pairs_wi = parse_m2_file(\"wi+locness/m2/ABCN.dev.gold.bea19.m2\")\n",
    "\n",
    "df_train_wi = pd.DataFrame(train_pairs_wi, columns=[\"input_text\", \"target_text\"])\n",
    "df_val_wi = pd.DataFrame(val_pairs_wi, columns=[\"input_text\", \"target_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93167be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "df_train_wi.to_csv(\"t5_train_wi.csv\", index=False, quoting = csv.QUOTE_ALL)\n",
    "df_val_wi.to_csv(\"t5_val_wi.csv\", index=False, quoting = csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa72cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing fce/m2/fce.train.gold.bea19.m2: 100%|██████████| 28350/28350 [00:00<00:00, 153812.60it/s]\n",
      "Parsing fce/m2/fce.dev.gold.bea19.m2: 100%|██████████| 2191/2191 [00:00<00:00, 150901.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_pairs_fce = parse_m2_file(\"fce/m2/fce.train.gold.bea19.m2\")\n",
    "val_pairs_fce = parse_m2_file(\"fce/m2/fce.dev.gold.bea19.m2\")\n",
    "\n",
    "\n",
    "df_train_fce= pd.DataFrame(train_pairs_fce, columns=[\"input_text\",\"target_text\"])\n",
    "df_val_fce= pd.DataFrame(val_pairs_fce, columns=[\"input_text\",\"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_fce.to_csv(\"t5_train_fce.csv\", index=False, quoting = csv.QUOTE_ALL)\n",
    "df_val_fce.to_csv(\"t5_val_fce.csv\", index=False, quoting = csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71512dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both training CSVs\n",
    "df1 = pd.read_csv(\"t5_train.csv\")\n",
    "df2 = pd.read_csv(\"t5_train_fce.csv\", skiprows=1)  # Skip header of second file\n",
    "\n",
    "# Concatenate them\n",
    "df_train_merged = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Optional: Drop duplicates\n",
    "df_train_merged = df_train_merged.drop_duplicates()\n",
    "\n",
    "# Save merged\n",
    "df_train_merged.to_csv(\"t5_train_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a08cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val1 = pd.read_csv(\"t5_val.csv\")\n",
    "df_val2 = pd.read_csv(\"t5_val_fce.csv\", skiprows=1)\n",
    "\n",
    "df_val_merged = pd.concat([df_val1, df_val2], ignore_index=True)\n",
    "df_val_merged = df_val_merged.drop_duplicates()\n",
    "\n",
    "df_val_merged.to_csv(\"t5_val_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"t5_train_merged.csv\") # our code was wrong above bc we accidentally stacked the csvs horizontally\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wl = pd.read_csv(\"t5_train.csv\")  # Should have proper headers\n",
    "df_fce = pd.read_csv(\"t5_train_fce.csv\", skiprows=1, names=[\"input_text\", \"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbbbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df_wl, df_fce], ignore_index=True)\n",
    "df_merged = df_merged.dropna()\n",
    "df_merged = df_merged.drop_duplicates()\n",
    "\n",
    "# Sanity check\n",
    "print(df_merged.columns)\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e409aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total rows:\", len(df_merged))\n",
    "print(\"Any nulls?\", df_merged.isnull().sum())\n",
    "print(\"Any duplicates?\", df_merged.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"t5_train_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wl_val = pd.read_csv(\"t5_val.csv\")  # Should have proper headers\n",
    "df_fce_val = pd.read_csv(\"t5_val_fce.csv\", skiprows=1, names=[\"input_text\", \"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda26f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df_wl_val, df_fce_val], ignore_index=True)\n",
    "df_merged = df_merged.dropna()\n",
    "df_merged = df_merged.drop_duplicates()\n",
    "\n",
    "# Sanity check\n",
    "print(df_merged.columns)\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df975fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total rows:\", len(df_merged))\n",
    "print(\"Any nulls?\", df_merged.isnull().sum())\n",
    "print(\"Any duplicates?\", df_merged.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"t5_val_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wl_len = pd.read_csv(\"t5_train_fce.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feedbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_wl_len))\n",
    "df_wl_len.drop_duplicates().dropna()\n",
    "print(len(df_wl_len))\n",
    "print(df_wl_len.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffa89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
